#%%

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns #optional

#%%
realtordata = pd.read_csv("realtor data.csv")
print(realtordata.columns)
#%%[markdown]
# Preprocessing
#%%
print("Few records from the dataset")
realtordata.head()
#%%
print("Descriptive statistics of numerical columns of Realtor data.")
realtordata.describe()
#%%
print("The total number of missing values in the dataset.")
print(realtordata.isnull().sum())
#%%[markdown]
# Dropping the prev_sold_date column as it does not contribute to the analysis.
#%%
realtordata.drop('prev_sold_date', axis=1, inplace=True)
#%%
print(realtordata.columns)

#%% [markdown]
# dropping data points where brokered_by,price,city,state,zip_code are missing as they constitute a very small number of the dataset.
#%%
realtordata_clean = realtordata.dropna(subset=['brokered_by','price','city','state','zip_code'])
#%%
print("Since the missing values in bed, bath, acre_lot and house_size are significantly high, the values will be imputed.")

#%%[markdown]
# Plotting the distribution of beds variable
#%%

plt.hist(realtordata_clean['bed'], bins=30, color='skyblue', edgecolor='black')
plt.yscale('log')
plt.title('Distribution of Bed count')
plt.xlabel('Value')
plt.ylabel('Log Frequency')
plt.show()

#%% [markdown]
# There are a few outliers in the bed count variable, hence removing outliers w.r.t bed count (only the upper bound).

#%%
upper_bound = 200
realtordata_no_outliers = realtordata_clean[(realtordata_clean['bed'] <= upper_bound)]

#%% [markdown]
# imputing a random value between the max and min of bed count.

#%%
random_values = np.random.uniform(realtordata_no_outliers['bed'].min(), realtordata_no_outliers['bed'].max(), size=realtordata_no_outliers['bed'].isna().sum())
realtordata_no_outliers['bed'].loc[realtordata_no_outliers['bed'].isna()] = random_values
print("The mean of bed variable before imputing:  ",realtordata_clean.describe()['bed']['mean'] )
print("The mean of bed variable after imputing:  ", realtordata_no_outliers.describe()['bed']['mean'])

#%%
# The mean of bed variable remains nearly same.

#%%[markdown]
# Plotting the distribution of bath variable
#%%

plt.hist(realtordata_clean['bath'], bins=40, color='skyblue', edgecolor='black')
plt.yscale('log')
plt.title('Distribution of Bath count')
plt.xlabel('Value')
plt.ylabel('Log Frequency')
plt.show()

#%% [markdown]
# There are a few outliers in the bath count variable, hence removing outliers w.r.t bath count (only the upper bound).

#%%
upper_bound = 300
realtordata_no_outliers = realtordata_no_outliers[(realtordata_no_outliers['bath'] <= upper_bound)]

#%% [markdown]
# Imputing a random value between the max and min of bath count.

#%%
random_values = np.random.uniform(realtordata_no_outliers['bath'].min(), realtordata_no_outliers['bath'].max(), size=realtordata_no_outliers['bath'].isna().sum())
realtordata_no_outliers['bath'].loc[realtordata_no_outliers['bath'].isna()] = random_values
print("The mean of bath variable before imputing:  ",realtordata_clean.describe()['bath']['mean'] )
print("The mean of bath variable after imputing:  ", realtordata_no_outliers.describe()['bath']['mean'])
#%%
# The mean of bath variable remains nearly same.

#%%


#%%[markdown]
# Plotting the distribution of acre_lot variable
#%%

plt.hist(realtordata_no_outliers['acre_lot'], bins=40, color='skyblue', edgecolor='black')
plt.yscale('log')
plt.title('Distribution of acre_lot')
plt.xlabel('Value')
plt.ylabel('Log Frequency')
plt.show()

#%% [markdown]
# Imputing the median value for acre_lot
#%%
median_value = realtordata_no_outliers['acre_lot'].median()
realtordata_no_outliers['acre_lot'].fillna(median_value, inplace=True)
print("The mean of acre_lot variable before imputing:  ",realtordata_clean.describe()['acre_lot']['mean'] )
print("The mean of acre_lot variable after imputing:  ", realtordata_no_outliers.describe()['acre_lot']['mean'])

#%%[markdown]
# Plotting the distribution of house_size variable
#%%

plt.hist(realtordata_no_outliers['house_size'], bins=40, color='skyblue', edgecolor='black')
plt.yscale('log')
plt.title('Distribution of house_size')
plt.xlabel('Value')
plt.ylabel('Log Frequency')
plt.show()

#%%
print(realtordata_no_outliers.shape[0])


#%% [markdown]
# Imputing the median value for house_size
#%%
median_value = realtordata_no_outliers['house_size'].median()
realtordata_no_outliers['house_size'].fillna(median_value, inplace=True)
print("The mean of house_size variable before imputing:  ",realtordata_clean.describe()['house_size']['mean'] )
print("The mean of house_size variable after imputing:  ", realtordata_no_outliers.describe()['house_size']['mean'])


########################################################
#
#%%
# Import necessary libraries
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

#%%
# Selecting the features for clustering
features = ['price', 'house_size', 'bed', 'bath', 'acre_lot']
data_for_clustering = realtordata_no_outliers[features]

#%%
# Splitting the data into training and testing sets using random sampling
train_data, test_data = train_test_split(data_for_clustering, test_size=0.2, random_state=42)

#%%
# Scaling the data
scaler = StandardScaler()
train_data_scaled = scaler.fit_transform(train_data)
test_data_scaled = scaler.transform(test_data)

#%%
# Sampling the data for manageable computation
sample_size = 100000  # Adjust sample size based on your system's capacity
if train_data.shape[0] > sample_size:
    train_data_sampled = train_data.sample(n=sample_size, random_state=42)
    train_data_scaled_sampled = scaler.fit_transform(train_data_sampled)
else:
    train_data_sampled = train_data
    train_data_scaled_sampled = train_data_scaled

#%%
# ----------- K-Means Clustering -----------
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(train_data_scaled_sampled)

# Adding K-Means cluster labels to the training dataframe
train_data_sampled['kmeans_cluster'] = kmeans_labels

# Silhouette score for K-Means on training data sample
kmeans_silhouette_train = silhouette_score(train_data_scaled_sampled, kmeans_labels)
print(f"K-Means Silhouette Score (Train, Sampled): {kmeans_silhouette_train:.2f}")

#%%
# ----------- DBSCAN Clustering -----------
dbscan = DBSCAN(eps=0.5, min_samples=10)
dbscan_labels = dbscan.fit_predict(train_data_scaled_sampled)

# Adding DBSCAN cluster labels to the training dataframe
train_data_sampled['dbscan_cluster'] = dbscan_labels

# Check unique clusters
unique_clusters = len(set(dbscan_labels) - {-1})  # Exclude noise (-1)
print(f"Number of DBSCAN clusters (excluding noise): {unique_clusters}")

# Silhouette score for DBSCAN (only if there are valid clusters)
if unique_clusters > 1:
    dbscan_silhouette_train = silhouette_score(train_data_scaled_sampled[dbscan_labels != -1],
                                               dbscan_labels[dbscan_labels != -1])
    print(f"DBSCAN Silhouette Score (Train, Sampled): {dbscan_silhouette_train:.2f}")
else:
    print("DBSCAN clustering did not form valid clusters.")

#%%
# ----------- Model Comparison -----------
print("\nModel Comparison (Train Data):")
model_scores_train = {
    "K-Means": kmeans_silhouette_train,
    "DBSCAN": dbscan_silhouette_train if 'dbscan_silhouette_train' in locals() else None
}

for model, score in model_scores_train.items():
    if score is not None:
        print(f"{model}: Silhouette Score = {score:.2f}")
    else:
        print(f"{model}: Not applicable")

# Selecting the best model based on the highest Silhouette Score for training data
best_model_train = max((score, model) for model, score in model_scores_train.items() if score is not None)
print(f"\nBest Model on Train Data: {best_model_train[1]} with Silhouette Score = {best_model_train[0]:.2f}")

#%%
# Visualization of Clusters
sns.scatterplot(
    x=train_data_sampled['house_size'],
    y=train_data_sampled['price'],
    hue=train_data_sampled['kmeans_cluster'],
    palette='viridis',
    legend='full'
)
plt.title('K-Means Clustering Visualization')
plt.xlabel('House Size')
plt.ylabel('Price')
plt.legend(title='Cluster')
plt.show()

sns.scatterplot(
    x=train_data_sampled['house_size'],
    y=train_data_sampled['price'],
    hue=dbscan_labels,
    palette='cool',
    legend='full'
)
plt.title('DBSCAN Clustering Visualization')
plt.xlabel('House Size')
plt.ylabel('Price')
plt.legend(title='Cluster')
plt.show()
