#%% [markdown]
# Importing necessary libraries
#%%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats


#%%[markdown]
# Loading the dataset 
#%%
plt.figure(figsize=(8, 6))
realtordata = pd.read_csv("realtor data.csv")
realtordata.head()

#%%[markdown]
# Preprocessing the data

#%%[markdown]
# Descriptive statistics of numerical columns of Realtor data.
#%%
realtordata[['price', 'bed', 'bath', 'acre_lot', 'house_size']].describe()

#%%[markdown]
# The total number of missing values in the dataset.
#%%
print(realtordata.isnull().sum())

#%%[markdown]
# Dropping the prev_sold_date column as it does not contribute to the analysis.
#%%
realtordata.drop(columns = ['prev_sold_date', 'street'], axis=1, inplace=True)

#%% [markdown]
# Dropping data points where brokered_by,price,city,state,zip_code are missing as they constitute a very small number of the dataset.
#%%
realtordata_clean = realtordata.dropna(subset=['brokered_by','price','city','state','zip_code'])

#%%[markdown]
# Since the missing values in bed, bath, acre_lot and house_size are significantly high, the values will be imputed.

#%%[markdown]
# Plotting the distribution of beds variable
#%%
plt.hist(realtordata_clean['bed'], bins=30, color='skyblue', edgecolor='black')
plt.yscale('log') 
plt.title('Distribution of Bed count')
plt.xlabel('Value')
plt.ylabel('Log Frequency')
plt.show()

#%% [markdown]
# There are evidently visible outliers in the bed count variable, hence removing outliers w.r.t bed count (only the upper bound).

#%%

Q1 = realtordata_clean['bed'].quantile(0.25)
Q3 = realtordata_clean['bed'].quantile(0.75)

# Calculate IQR
IQR = Q3 - Q1
upper_bound = Q3 + 1.5 * IQR
realtordata_no_outliers = realtordata_clean[(realtordata_clean['bed'] <= upper_bound)]

#%% [markdown]
# Imputing a random value between the max and min of bed count variable.
#%%
random_values = np.random.uniform(realtordata_no_outliers['bed'].min(), realtordata_no_outliers['bed'].max(), size=realtordata_no_outliers['bed'].isna().sum())
realtordata_no_outliers['bed'].loc[realtordata_no_outliers['bed'].isna()] = random_values
#%%
print("The mean of bed variable before imputing:  ",realtordata_clean.describe()['bed']['mean'] )
print("The mean of bed variable after imputing:  ", realtordata_no_outliers.describe()['bed']['mean'])

#%% [markdown]
# Plotting the distribution of beds variable after removing the outliers.
#%%
plt.hist(realtordata_no_outliers['bed'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Bed count')
plt.xlabel('Value')
plt.ylabel('Log Frequency')
plt.show()

#%% [markdown]
# The mean of bed variable remains nearly same.

#%%[markdown]
# Plotting the distribution of bath variable
#%%
plt.hist(realtordata_clean['bath'], bins=40, color='skyblue', edgecolor='black')
plt.yscale('log')
plt.title('Distribution of Bath count')
plt.xlabel('Value')
plt.ylabel('Log Frequency')
plt.show()

#%% [markdown]
# There are a few outliers in the bath count variable, hence removing outliers w.r.t bath count (only the upper bound).
#%%
Q1 = realtordata_no_outliers['bath'].quantile(0.25)
Q3 = realtordata_no_outliers['bath'].quantile(0.75)
IQR = Q3 - Q1
upper_bound = Q3 + 1.5 * IQR

realtordata_no_outliers = realtordata_no_outliers[(realtordata_no_outliers['bath'] <= upper_bound)]
#%% [markdown]
# Imputing a random value between the max and min of bath variable count.

#%%
random_values = np.random.uniform(realtordata_no_outliers['bath'].min(), realtordata_no_outliers['bath'].max(), size=realtordata_no_outliers['bath'].isna().sum())
realtordata_no_outliers['bath'].loc[realtordata_no_outliers['bath'].isna()] = random_values
#%%
print("The mean of bath variable before imputing:  ",realtordata_clean.describe()['bath']['mean'] )
print("The mean of bath variable after imputing:  ", realtordata_no_outliers.describe()['bath']['mean'])

#%%[markdown]
# Plotting the distribution of bath variable after removing the outliers
#%%
plt.hist(realtordata_no_outliers['bath'], bins=40, color='skyblue', edgecolor='black')
plt.title('Distribution of Bath count')
plt.xlabel('Value')
plt.ylabel('Log Frequency')
plt.show()
#%%[markdown]
# The mean of bath variable remains nearly same.

#%%[markdown]
# Plotting the distribution of acre_lot variable
#%%
plt.hist(realtordata_no_outliers['acre_lot'], bins=40, color='skyblue', edgecolor='black')
plt.yscale('log')
plt.title('Distribution of acre_lot')
plt.xlabel('Value')
plt.ylabel('Log Frequency')
plt.show()
#%%
print("The mean of acre_lot variable before imputing:  ",realtordata_no_outliers.describe()['acre_lot']['mean'] )

#%% [markdown]
# Imputing the mean value for acre_lot
#%%
mean_value = realtordata_no_outliers['acre_lot'].mean()
realtordata_no_outliers['acre_lot'].fillna(mean_value, inplace=True)
#%%
print("The mean of acre_lot variable after imputing:  ", realtordata_no_outliers.describe()['acre_lot']['mean'])

#%%[markdown]
# Plotting the distribution of house_size variable
#%%
plt.hist(realtordata_no_outliers['house_size'], bins=40, color='skyblue', edgecolor='black')
plt.yscale('log')
plt.title('Distribution of house_size')
plt.xlabel('Value')
plt.ylabel('Log Frequency')
plt.show()
#%%
print("The mean of house_size variable before imputing:  ",realtordata_no_outliers.describe()['house_size']['mean'] )

#%% [markdown]
# Imputing the median value for house_size
#%%
mean_value = realtordata_no_outliers['house_size'].mean()
realtordata_no_outliers['house_size'].fillna(mean_value, inplace=True)
#%%
print("The mean of house_size variable after imputing:  ", realtordata_no_outliers.describe()['house_size']['mean'])

#%% [markdown]
#####################################
Market Segmentation
#################################
#%%
# Import necessary libraries
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# %% [markdown]
# Sampling the data for manageable computation
# %%
sample_size = 100000  # Define sample size
if realtordata_no_outliers.shape[0] > sample_size:
    sampled_data = realtordata_no_outliers.sample(n=sample_size, random_state=42).reset_index(drop=True)
else:
    sampled_data = realtordata_no_outliers
#%%
# Standardize the features for clustering
features = ['price', 'house_size', 'bed', 'bath', 'acre_lot']
scaler = StandardScaler()
data_scaled_sampled = scaler.fit_transform(sampled_data[features])
#%%
# Determine the optimal number of clusters using Elbow Method
silhouette_scores = []
for k in range(2, 10):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(data_scaled_sampled)
    silhouette_scores.append(silhouette_score(data_scaled_sampled, labels))
#%%
# Plot silhouette scores
plt.plot(range(2, 10), silhouette_scores, marker='o')
plt.title("Elbow Method with Silhouette Score")
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.show()
#%%
# Apply K-Means with the optimal number of clusters
optimal_clusters = 3  # Replace with the actual optimal number
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
sampled_data['kmeans_cluster'] = kmeans.fit_predict(data_scaled_sampled)
#%%
# Composite scoring for categorization
centroids = pd.DataFrame(kmeans.cluster_centers_, columns=features)
weights = {'price': 0.4, 'house_size': 0.3, 'bed': 0.1, 'bath': 0.1, 'acre_lot': 0.1}
centroids['composite_score'] = (
    centroids['price'] * weights['price'] +
    centroids['house_size'] * weights['house_size'] +
    centroids['bed'] * weights['bed'] +
    centroids['bath'] * weights['bath'] +
    centroids['acre_lot'] * weights['acre_lot']
)
#%%
# Assign cluster categories
sorted_centroids = centroids.sort_values(by='composite_score').reset_index()
cluster_mapping = {sorted_centroids.iloc[i].name: label for i, label in enumerate(['Affordable', 'Mid-Range', 'High-End'])}
sampled_data['category'] = sampled_data['kmeans_cluster'].map(cluster_mapping)
#%%
# Visualization
sns.scatterplot(
    x=sampled_data['house_size'],
    y=sampled_data['price'],
    hue=sampled_data['category'],
    palette='coolwarm',
    legend='full'
)
plt.title('K-Means Clustering on Sampled Data: Price vs House Size')
plt.xlabel('House Size')
plt.ylabel('Price')
plt.legend(title='Category')
plt.show()
#%%
# Evaluate Silhouette Score on Sampled Data
silhouette_avg = silhouette_score(data_scaled_sampled, sampled_data['kmeans_cluster'])
print(f"Silhouette Score for K-Means Clustering (Sampled Data): {silhouette_avg:.2f}")

#%% [markdown]
silhouette score of 0.20 indicates modest cluster separation 
##############################
#########################################
